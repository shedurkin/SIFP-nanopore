---
title: "03.01-G4-Library1-MinION-Dorado-recall-GPU"
author: "Kathleen Durkin"
date: "2025-10-29"
always_allow_html: true
output: 
  github_document:
    toc: true
    toc_depth: 3
    number_sections: true
    html_preview: true 
  bookdown::html_document2:
    theme: cosmo
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show
    code_download: true
---

I've been trialing Dorado basecalling to get my raw pod5 files to basecalled, demultiplexed, trimmed, and aligned BAM files, However, this is very computationally intensive and is taking hours to process 50-200Mb of output using CPUs along. `Dorado` is, however, optemized to run on GPUs, which would significantly cut run time!

I'm going to try using a Hyak GPU to run Dorado basecalling, using [Sam's previous work](https://robertslab.github.io/sams-notebook/posts/2020/2020-09-04-Data-Wrangling---NanoPore-Fast5-Conversion-to-FastQ-of-C.bairdi-6129_403_26-on-Mox-with-GPU-Node/index.html) and the [Dorado basecaller documentation](https://software-docs.nanoporetech.com/dorado/latest/basecaller/basecall_overview/) as reference. 

```{r setup, include=FALSE}
library(knitr)
library(tidytable)
library(tidyverse)
knitr::opts_chunk$set(
  echo = TRUE,         # Display code chunks
  eval = FALSE,        # Evaluate code chunks
  warning = FALSE,     # Hide warnings
  message = FALSE,     # Hide messages
  comment = ""         # Prevents appending '##' to beginning of lines in code output
)
```


# Create a Bash variables file

This allows usage of Bash variables (e.g. paths to common directories) across R Markdown chunks.
```{r save-bash-variables-to-rvars-file, engine='bash', eval=TRUE}
{
echo "#### Assign Variables ####"
echo ""

echo "# Data directories"
echo 'export nanopore_dir=/gscratch/srlab/kdurkin1/SIFP-nanopore'
echo 'export output_dir_top=${nanopore_dir}/D-Group4/output/03.01-G4-Library1-MinION-Dorado-recall-GPU'
echo 'export data_dir_top=${nanopore_dir}/D-Group4/data/03.01-G4-Library1-MinION-Dorado-recall-GPU'
echo 'export raw_pod5_dir=${data_dir_top}/pod5_pass'
echo 'export raw_pod5_url1="https://gannet.fish.washington.edu/kdurkin1/SIFP_2025/Group4_MinION/Library1/20250905_1158_MD-101223_FBD08455_b87a1f92/pod5_pass/"'
echo 'export raw_pod5_url2="https://gannet.fish.washington.edu/kdurkin1/SIFP_2025/Group4_MinION/Library1_wash3/20250907_1505_MD-101223_FBD08455_50a8f15e/pod5_pass/"'
echo 'export genome_dir=${nanopore_dir}/data/GCA_965233905.1_jaEunKnig1.1/'

echo 'export samtools=/srlab/programs/samtools-1.20/samtools'

echo "# Set FastQ filename patterns"
echo "export pod5_pattern='*.pod5'"
echo ""

echo "# Set number of CPUs to use"
echo 'export threads=20'
echo ""

echo "# Input/output files"
echo 'export raw_checksums=checksums.md5'
echo 'export trimmed_checksums=trimmed_fastq_checksums.md5'
echo ""
} > .bashvars

cat .bashvars
```

# Raw reads
## Download raw pod5 files

Reads are downloaded from: 
https://gannet.fish.washington.edu/kdurkin1/SIFP_2025/Group4_MinION/Library1/20250905_1158_MD-101223_FBD08455_b87a1f92/pod5_pass/ and https://gannet.fish.washington.edu/kdurkin1/SIFP_2025/Group4_MinION/Library1_wash3/20250907_1505_MD-101223_FBD08455_50a8f15e/pod5_pass/

Note that there are two directories of data for the Group 4 Library 1 MinION runs. One contains the bulk of the run, the second is everything after wash 3 (the run errored during wash 3)

Note that this directory contains multiple subdirectories, each representing one barcode (specimen) and containing the pod5 files associated with that barcode

The `--cut-dirs 7` command cuts the preceding directory structure (i.e. `nightingales/P_evermanni/30-789513166/`) so that we just end up with the reads.

```{r download-raw-reads, engine='bash', eval=FALSE}
# Load bash variables into memory
source .bashvars

# pre-wash3
wget \
--directory-prefix ${raw_pod5_dir}/pre_wash3 \
--recursive \
--no-check-certificate \
--continue \
--cut-dirs 6 \
--no-host-directories \
--no-parent \
--quiet \
--accept ${pod5_pattern} ${raw_pod5_url1}

# post-wash3
wget \
--directory-prefix ${raw_pod5_dir}/post_wash3 \
--recursive \
--no-check-certificate \
--continue \
--cut-dirs 6 \
--no-host-directories \
--no-parent \
--quiet \
--accept ${pod5_pattern} ${raw_pod5_url2}
```

```{r check-raw-reads, engine='bash', eval=TRUE}
# Load bash variables into memory
source .bashvars

ls -lh "${raw_pod5_dir}"
```

## Verify raw read checksums
```{r verify-raw-read-checksums, engine='bash', eval=FALSE}
# Load bash variables into memory
source .bashvars

wget \
--directory-prefix ${raw_pod5_dir}/pre_wash3 \
--recursive \
--no-check-certificate \
--continue \
--cut-dirs 6 \
--no-host-directories \
--no-parent \
--quiet \
--accept 'checksums.md5' ${raw_pod5_url1}

cd "${raw_pod5_dir}/pre_wash3"

# Recursively verify checksums in all subdirectories
find $(pwd) -type d | while read -r DIR; do
    # Check if checksums.md5 exists in this directory
    if [[ -f "$DIR/checksums.md5" ]]; then
        echo "Verifying checksums in $DIR"
        (
            cd "$DIR" || exit 1
            md5sum -c checksums.md5
        )
        echo ""
    fi
done
```

I've taken the SLURM script Sam made to run Guppy using `ckpt` GPU resources and modified it to use the dorado basecaller (available on Klone in the contatiner `/gscratch/srlab/containers/srlab-R4.4-bioinformatics-container-3886a1c.sif`). The modified script is saved in this directory under the name `03.01_G4L1_MinION_Dorado.sh`.

To run the script, run the below command from Klone terminal:
```{r, engine='bash', eval=FALSE}
sbatch 03.01_G4L1_MinION_Dorado.sh
```

Job ID 31837837, finished in 1hr 27min

# BAM processing

Separate out mapped reads, and separate by barcode
```{r, engine='bash', eval=FALSE}
source .bashvars

cd ${output_dir_top}

# Alignment summary
$samtools flagstat -O tsv FBD08455_pass_recalled.bam > FBD08455_pass_recalled_summary.tsv
```

```{r, engine='bash', eval=TRUE}
source .bashvars
cd ${output_dir_top}
cat FBD08455_pass_recalled_summary.tsv
```

From the samtools `flagstat` summary we see a high alignment rate of primary reads (45.18%), which is ok. A large portion of reads confidently come from *Eunicea*. There are, however, a lot of "secondary" and "supplementary" reads. "Secondary" indicates secondary alignments, i.e. alternative mapping locations of the same read. "Supplementary" indicates supplementary alignments, or parts of a read that align *discontinuously* (possibly chimeric, split, large structural variants, etc.). The true number of reads present in my sample is just the *primary* alignments (total - secondary - supplementary = 3950904-1976324-101090 = 1,873,490 reads). This is a rather annoying artifact of Dorado using a multimapper under the hood, where a single read can be mapped to multiple possible loci (one primary and 0 or more secondary). For downstream work, we only want to be using the "real" reads, not duplicate records of secondary/supplementary alignments, so we need to filter those out of the BAM. We also need to sort them

```{r, engine='bash', eval=FALSE}
source .bashvars

cd ${output_dir_top}

# Remove unmapped reads (bit decimal 4), secondary alignments (bit decimal 256) and supplementary alignments (bit decimal 2048)
$samtools view -b -F 2308 FBD08455_pass_recalled.bam |\
$samtools sort -o FBD08455_pass_recalled_mapped.bam

# Separate by barcode
$samtools view -h FBD08455_pass_recalled_mapped.bam |\
awk '/^@/ || /BC:Z:SQK-NBD114-96_barcode27/' |\
$samtools view -b |\
$samtools sort -o FBD08455_pass_recalled_mapped_barcode27.bam

$samtools view -h FBD08455_pass_recalled_mapped.bam |\
awk '/^@/ || /BC:Z:SQK-NBD114-96_barcode28/' |\
$samtools view -b |\
$samtools sort -o FBD08455_pass_recalled_mapped_barcode28.bam

$samtools view -h FBD08455_pass_recalled_mapped.bam |\
awk '/^@/ || /BC:Z:SQK-NBD114-96_barcode29/' |\
$samtools view -b |\
$samtools sort -o FBD08455_pass_recalled_mapped_barcode29.bam
```

Index
```{r, engine='bash', eval=FALSE}
source .bashvars
cd ${output_dir_top}

$samtools index FBD08455_pass_recalled_mapped.bam
$samtools index FBD08455_pass_recalled_mapped_barcode27.bam
$samtools index FBD08455_pass_recalled_mapped_barcode28.bam
$samtools index FBD08455_pass_recalled_mapped_barcode29.bam

```


# Summary stats

```{r, engine='bash', eval=FALSE}
source .bashvars

cd ${output_dir_top}

# Alignment summary
$samtools flagstat -O tsv FBD08455_pass_recalled_mapped.bam > flagstat_summary_FBD08455_pass_recalled_mapped.tsv
```

```{r, engine='bash', eval=TRUE}
source .bashvars
cd ${output_dir_top}

cat flagstat_summary_FBD08455_pass_recalled_mapped.tsv

echo ""

# Summarize by barcode
$samtools view FBD08455_pass_recalled_mapped.bam | \
awk '{for(i=12;i<=NF;i++) if($i ~ /^BC:Z:/) {print substr($i,6)}}' | \
sort | uniq -c | sort -nr
```

There's a discrepancy among the barcodes. The barcodes 27, 28, and 29 have ~0.2M, ~0.15M and ~0.45M reads, respectively.

## modkit modification summaries

Summarize modifications
```{r, engine='bash', eval=FALSE}
source .bashvars
cd ${output_dir_top}

$modkit summary -n 100000 FBD08455_pass_recalled_mapped.bam
```

OR, if doing this from Klone, need to use conda (run from terminal)

```{r, engine='bash', eval=FALSE}
source .bashvars
cd ${output_dir_top}

conda create -n modkit_env -c bioconda -c conda-forge ont-modkit
conda activate modkit_env

modkit summary -n 100000 FBD08455_pass_recalled_mapped.bam
```

```{}
> sampling 100000 reads from BAM
> calculating threshold at 10(th) percentile
> calculated thresholds: A: 0.7792969 C: 0.5839844
# bases             C,A 
# total_reads_used  77116 
# count_reads_C     71819 
# count_reads_A     77113 
# pass_threshold_A  0.7792969 
# pass_threshold_C  0.5839844 
 base  code  pass_count  pass_frac    all_count  all_frac 
 C     -     505966      0.9030076    529701     0.85084057 
 C     h     22399       0.03997594   44040      0.07073994 
 C     m     31947       0.057016447  48821      0.0784195 
 A     -     5671175     0.9877967    6150199    0.96493477 
 A     a     70062       0.012203294  223495     0.035065223 
```

We're primarily interested in the passed read stats (though comparing to the "all" read stats can be useful). Even though only reads that passed the Nanopore quality filter were included in the modkit summary, there is still additional filtering happening in the summary process. That's because Nanopore quality filtering is read-level, and the modkit quality filtering is base-level. 

In mapped, primary-alignment reads, we observe low 6mA (~1%), low 5hmCG (~4%), and moderately-low 5mCG (~6%). Notably, these are all higher than in G1L4 (the modern samples)

Now let's check by barcode (individual)

```{r, engine='bash', eval=FALSE}
source .bashvars

cd ${output_dir_top}

$modkit summary -n 100000 --tsv FBD08455_pass_recalled_mapped_barcode27.bam > modkit_summary_FBD08455_barcode27.tsv

echo ""

$modkit summary -n 100000 --tsv FBD08455_pass_recalled_mapped_barcode28.bam > modkit_summary_FBD08455_barcode28.tsv

echo ""

$modkit summary -n 100000 --tsv FBD08455_pass_recalled_mapped_barcode29.bam > modkit_summary_FBD08455_barcode29.tsv
```

OR, again, use a conda environment

Barplot of above modkit summaries
```{r, eval=TRUE}

files <- c(
  "../output/03.01-G4-Library1-MinION-Dorado-recall-GPU/modkit_summary_FBD08455_barcode27.tsv",
  "../output/03.01-G4-Library1-MinION-Dorado-recall-GPU/modkit_summary_FBD08455_barcode28.tsv",
  "../output/03.01-G4-Library1-MinION-Dorado-recall-GPU/modkit_summary_FBD08455_barcode29.tsv"
)

# Assign sample names (in same order as files)
sample_names <- c("barcode27", "barcode28", "barcode29")

# parse summaries
parse_modkit_summary <- function(file, sample_name) {
  df <- read.table(file, sep = "\t", header = FALSE, stringsAsFactors = FALSE)
  
  # Extract values (use pattern matching on the first column)
  get_val <- function(pattern) {
    as.numeric(df$V2[grep(pattern, df$V1)])
  }
  
  tibble(
    Sample = sample_name,
    `5mC`  = get_val("^C_pass_frac_modified_m") * 100,
    `5hmC` = get_val("^C_pass_frac_modified_h") * 100,
    `6mA`  = get_val("^A_pass_frac_modified_a") * 100
  )
}

# combine and convert to long format (for ggplotting)
mod_data <- map2_dfr(files, sample_names, parse_modkit_summary)

mod_long <- mod_data %>%
  pivot_longer(cols = c(`5mC`, `5hmC`, `6mA`),
               names_to = "Modification",
               values_to = "Percent")

# plot
ggplot(mod_long, aes(x = Sample, y = Percent, fill = Modification)) +
  geom_bar(stat = "identity", width=0.5, position = position_dodge(width = 0.5)) +
  geom_text(aes(label = sprintf("%.1f", Percent)),     # format to one decimal place
            position = position_dodge(width = 0.5), 
            vjust = -0.5, size = 4) +                  # adjust spacing & text size
  labs(
    title = "Modified Base Composition per Sample (trimmed, mapped reads)",
    y = "% Modified Bases (Pass Fraction)",
    x = NULL
  )

```

## genome coverage summaries

```{r, engine='bash', eval=FALSE}
source .bashvars
cd ${output_dir_top}

# conda create -n myenv
conda activate mosdepth_env
conda config --add channels bioconda
conda config --add channels conda-forge
conda config --add channels defaults
conda install -c bioconda mosdepth
mosdepth --version

```

