---
title: "04.01-G2-Library2-MinION-Dorado-recall-GPU"
author: "Kathleen Durkin"
date: "2025-10-29"
always_allow_html: true
output: 
  github_document:
    toc: true
    toc_depth: 3
    number_sections: true
    html_preview: true 
  bookdown::html_document2:
    theme: cosmo
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show
    code_download: true
---

I've been trialing Dorado basecalling to get my raw pod5 files to basecalled, demultiplexed, trimmed, and aligned BAM files, However, this is very computationally intensive and is taking hours to process 50-200Mb of output using CPUs along. `Dorado` is, however, optemized to run on GPUs, which would significantly cut run time!

I'm going to try using a Hyak GPU to run Dorado basecalling, using [Sam's previous work](https://robertslab.github.io/sams-notebook/posts/2020/2020-09-04-Data-Wrangling---NanoPore-Fast5-Conversion-to-FastQ-of-C.bairdi-6129_403_26-on-Mox-with-GPU-Node/index.html) and the [Dorado basecaller documentation](https://software-docs.nanoporetech.com/dorado/latest/basecaller/basecall_overview/) as reference. 

```{r setup, include=FALSE}
library(knitr)
library(tidytable)
library(tidyverse)
knitr::opts_chunk$set(
  echo = TRUE,         # Display code chunks
  eval = FALSE,        # Evaluate code chunks
  warning = FALSE,     # Hide warnings
  message = FALSE,     # Hide messages
  comment = ""         # Prevents appending '##' to beginning of lines in code output
)
```


# Create a Bash variables file

This allows usage of Bash variables (e.g. paths to common directories) across R Markdown chunks.
```{r save-bash-variables-to-rvars-file, engine='bash', eval=TRUE}
{
echo "#### Assign Variables ####"
echo ""

echo "# Data directories"
echo 'export nanopore_dir=/gscratch/srlab/kdurkin1/SIFP-nanopore'
echo 'export output_dir_top=${nanopore_dir}/B-Group2/output/04.01-G2-Library2-MinION-Dorado-recall-GPU'
echo 'export data_dir_top=${nanopore_dir}/B-Group2/data/04.01-G2-Library2-MinION-Dorado-recall-GPU'
echo 'export raw_pod5_dir=${data_dir_top}/pod5_pass'
echo 'export raw_pod5_url1="https://gannet.fish.washington.edu/kdurkin1/SIFP_2025/Group2_MinION/Library2/20250824_2226_MD-101223_FBD39370_195b7d5f/pod5_pass/"'
echo 'export raw_pod5_url2="https://gannet.fish.washington.edu/kdurkin1/SIFP_2025/Group2_MinION/Library2_post_update/20250826_1059_MD-101223_FBD39370_b38d8279/pod5_pass/"'
echo 'export genome_dir=${nanopore_dir}/data/GCA_965233905.1_jaEunKnig1.1/'

echo 'export samtools=/srlab/programs/samtools-1.20/samtools'


echo "# Set FastQ filename patterns"
echo "export pod5_pattern='*.pod5'"
echo ""

echo "# Set number of CPUs to use"
echo 'export threads=20'
echo ""

echo "# Input/output files"
echo 'export raw_checksums=checksums.md5'
echo 'export trimmed_checksums=trimmed_fastq_checksums.md5'
echo ""
} > .bashvars

cat .bashvars
```

# Raw reads
## Download raw pod5 files

Reads are downloaded from: 
https://gannet.fish.washington.edu/kdurkin1/SIFP_2025/Group2_MinION/Library2/20250824_2226_MD-101223_FBD39370_195b7d5f/pod5_pass/ and https://gannet.fish.washington.edu/kdurkin1/SIFP_2025/Group2_MinION/Library2_post_update/20250826_1059_MD-101223_FBD39370_b38d8279/pod5_pass/

Note that there are two directories of data for the Group 4 Library 1 MinION runs. One contains the bulk of the run, the second is everything after a forced computer update

Note that this directory contains multiple subdirectories, each representing one barcode (specimen) and containing the pod5 files associated with that barcode

The `--cut-dirs 7` command cuts the preceding directory structure (i.e. `nightingales/P_evermanni/30-789513166/`) so that we just end up with the reads.

```{r download-raw-reads, engine='bash', eval=FALSE}
# Load bash variables into memory
source .bashvars

# pre-update
wget \
--directory-prefix ${raw_pod5_dir}/pre_update \
--recursive \
--no-check-certificate \
--continue \
--cut-dirs 6 \
--no-host-directories \
--no-parent \
--quiet \
--accept ${pod5_pattern} ${raw_pod5_url1}

# post-update
wget \
--directory-prefix ${raw_pod5_dir}/post_update \
--recursive \
--no-check-certificate \
--continue \
--cut-dirs 6 \
--no-host-directories \
--no-parent \
--quiet \
--accept ${pod5_pattern} ${raw_pod5_url2}
```

```{r check-raw-reads, engine='bash', eval=TRUE}
# Load bash variables into memory
source .bashvars

ls -lh "${raw_pod5_dir}"
```

## Verify raw read checksums
```{r verify-raw-read-checksums, engine='bash', eval=FALSE}
# Load bash variables into memory
source .bashvars

wget \
--directory-prefix ${raw_pod5_dir}/pre_update \
--recursive \
--no-check-certificate \
--continue \
--cut-dirs 6 \
--no-host-directories \
--no-parent \
--quiet \
--accept 'checksums.md5' ${raw_pod5_url1}

cd "${raw_pod5_dir}/pre_update"

# Recursively verify checksums in all subdirectories
find $(pwd) -type d | while read -r DIR; do
    # Check if checksums.md5 exists in this directory
    if [[ -f "$DIR/checksums.md5" ]]; then
        echo "Verifying checksums in $DIR"
        (
            cd "$DIR" || exit 1
            md5sum -c checksums.md5
        )
        echo ""
    fi
done
```

I've taken the SLURM script Sam made to run Guppy using `ckpt` GPU resources and modified it to use the dorado basecaller (available on Klone in the contatiner `/gscratch/srlab/containers/srlab-R4.4-bioinformatics-container-3886a1c.sif`). The modified script is saved in this directory under the name `04.01_G2L2_MinION_Dorado.sh`.

To run the script, run the below command from Klone terminal:
```{r, engine='bash', eval=FALSE}
sbatch 04.01_G2L2_MinION_Dorado.sh
```

Job ID 30552393



# BAM processing

Separate out mapped reads, and separate by barcode
```{r, engine='bash', eval=FALSE}
source .bashvars

cd ${output_dir_top}

# Alignment summary
$samtools flagstat -O tsv FBD39370_pass_recalled.bam > FBD39370_pass_recalled_summary.tsv
```

```{r, engine='bash', eval=TRUE}
source .bashvars

cd ${output_dir_top}
cat FBD39370_pass_recalled_summary.tsv
```

From the samtools `flagstat` summary we see a high alignment rate of primary reads (61.62%), which is great! The vast majority of reads confidently come from *Eunicea*. There are, however, a lot of "secondary" and "supplementary" reads. "Secondary" indicates secondary alignments, i.e. alternative mapping locations of the same read. "Supplementary" indicates supplementary alignments, or parts of a read that align *discontinuously* (possibly chimeric, split, large structural variants, etc.). The true number of reads present in my sample is just the *primary* alignments (total - secondary - supplementary = 9629255-5404041-310786 = 3,914,428 reads). This is a rather annoying artifact of Dorado using a multimapper under the hood, where a single read can be mapped to multiple possible loci (one primary and 0 or more secondary). For downstream work, we only want to be using the "real" reads, not duplicate records of secondary/supplementary alignments, so we need to filter those out of the BAM. We also need to sort them

```{r, engine='bash', eval=FALSE}
source .bashvars

cd ${output_dir_top}

# Remove unmapped reads (bit decimal 4), secondary alignments (bit decimal 256) and supplementary alignments (bit decimal 2048)
$samtools view -b -F 2308 FBD39370_pass_recalled.bam |\
$samtools sort -o FBD39370_pass_recalled_mapped.bam

# Separate by barcode
$samtools view -h FBD39370_pass_recalled_mapped.bam |\
awk '/^@/ || /BC:Z:SQK-NBD114-96_barcode18/' |\
$samtools view -b |\
$samtools sort -o FBD39370_pass_recalled_mapped_barcode18.bam

$samtools view -h FBD39370_pass_recalled_mapped.bam |\
awk '/^@/ || /BC:Z:SQK-NBD114-96_barcode19/' |\
$samtools view -b |\
$samtools sort -o FBD39370_pass_recalled_mapped_barcode19.bam

$samtools view -h FBD39370_pass_recalled_mapped.bam |\
awk '/^@/ || /BC:Z:SQK-NBD114-96_barcode20/' |\
$samtools view -b |\
$samtools sort -o FBD39370_pass_recalled_mapped_barcode20.bam
```

Index
```{r, engine='bash', eval=FALSE}
source .bashvars
cd ${output_dir_top}

$samtools index FBD39370_pass_recalled_mapped.bam
$samtools index FBD39370_pass_recalled_mapped_barcode18.bam
$samtools index FBD39370_pass_recalled_mapped_barcode19.bam
$samtools index FBD39370_pass_recalled_mapped_barcode20.bam

```


# Summary stats

```{r, engine='bash', eval=TRUE}
source .bashvars

cd ${output_dir_top}

# Alignment summary
$samtools flagstat -O tsv FBD39370_pass_recalled_mapped.bam > flagstat_summary_FBD39370_pass_recalled_mapped.tsv
```

```{r, engine='bash', eval=FALSE}
source .bashvars

cd ${output_dir_top}

cat flagstat_summary_FBD39370_pass_recalled_mapped.tsv

echo ""

# Summarize by barcode
$samtools view FBD39370_pass_recalled_mapped.bam | \
awk '{for(i=12;i<=NF;i++) if($i ~ /^BC:Z:/) {print substr($i,6)}}' | \
sort | uniq -c | sort -nr
```

There's a bit of a discrepancy among the barcodes, but it's not terrible! Barcodes 19 and 20 each have 0.6-0.7M reads, while Barcode 18 has almost 1M.

## modkit modification summaries

Summarize modifications
```{r, engine='bash', eval=FALSE}
source .bashvars

$modkit summary -n 100000 ${output_dir_top}/FBD39370_pass_recalled_mapped.bam
```

OR, if doing this from Klone, need to use conda (run from terminal)

```{r, engine='bash', eval=FALSE}
source .bashvars

conda create -n modkit_env -c bioconda -c conda-forge ont-modkit
conda activate modkit_env

modkit summary -n 100000 ${output_dir_top}/FBD39370_pass_recalled_mapped.bam
```

```{}
> sampling 100000 reads from BAM
> calculating threshold at 10(th) percentile
> calculated thresholds: A: 0.8730469 C: 0.8125
# bases             A,C 
# total_reads_used  77481 
# count_reads_A     77480 
# count_reads_C     74203 
# pass_threshold_C  0.8125 
# pass_threshold_A  0.8730469 
 base  code  pass_count  pass_frac    all_count  all_frac 
 A     -     7270966     0.99379295   7874355    0.97050965 
 A     a     45413       0.006207032  239274     0.02949038 
 C     -     599000      0.9389774    643764     0.90982246 
 C     h     5685        0.008911664  20510      0.028986491 
 C     m     33243       0.052110896  43297      0.061191034
```

We're primarily interested in the passed read stats (though comparing to the "all" read stats can be useful). Even though only reads that passed the Nanopore quality filter were included in the modkit summary, there is still additional filtering happening in the summary process. That's because Nanopore quality filtering is read-level, and the modkit quality filtering is base-level. 
In mapped, primary-alignment reads, we observe low/negligible 6mA (~0.6%), low/negligible 5hmCG (~0.9%), and moderately-low 5mCG (~5%).

Now let's check by barcode (individual)

```{r, engine='bash', eval=FALSE}
source .bashvars

cd ${output_dir_top}

$modkit summary -n 100000 --tsv FBD39370_pass_recalled_mapped_barcode18.bam > modkit_summary_FBD39370_barcode18.tsv

echo ""

$modkit summary -n 100000 --tsv FBD39370_pass_recalled_mapped_barcode19.bam > modkit_summary_FBD39370_barcode19.tsv

echo ""

$modkit summary -n 100000 --tsv FBD39370_pass_recalled_mapped_barcode20.bam > modkit_summary_FBD39370_barcode20.tsv
```

OR, again, use a conda environment

Barplot of above modkit summaries
```{r, eval=TRUE}

files <- c(
  "../output/04.01-G2-Library2-MinION-Dorado-recall-GPU/modkit_summary_FBD39370_barcode18.tsv",
  "../output/04.01-G2-Library2-MinION-Dorado-recall-GPU/modkit_summary_FBD39370_barcode19.tsv",
  "../output/04.01-G2-Library2-MinION-Dorado-recall-GPU/modkit_summary_FBD39370_barcode20.tsv"
)

# Assign sample names (in same order as files)
sample_names <- c("barcode18", "barcode19", "barcode20")

# parse summaries
parse_modkit_summary <- function(file, sample_name) {
  df <- read.table(file, sep = "\t", header = FALSE, stringsAsFactors = FALSE)
  
  # Extract values (use pattern matching on the first column)
  get_val <- function(pattern) {
    as.numeric(df$V2[grep(pattern, df$V1)])
  }
  
  tibble(
    Sample = sample_name,
    `5mC`  = get_val("^C_pass_frac_modified_m") * 100,
    `5hmC` = get_val("^C_pass_frac_modified_h") * 100,
    `6mA`  = get_val("^A_pass_frac_modified_a") * 100
  )
}

# combine and convert to long format (for ggplotting)
mod_data <- map2_dfr(files, sample_names, parse_modkit_summary)

mod_long <- mod_data %>%
  pivot_longer(cols = c(`5mC`, `5hmC`, `6mA`),
               names_to = "Modification",
               values_to = "Percent")

# plot
ggplot(mod_long, aes(x = Sample, y = Percent, fill = Modification)) +
  geom_bar(stat = "identity", width=0.5, position = position_dodge(width = 0.5)) +
  geom_text(aes(label = sprintf("%.1f", Percent)),     # format to one decimal place
            position = position_dodge(width = 0.5), 
            vjust = -0.5, size = 4) +                  # adjust spacing & text size
  labs(
    title = "Modified Base Composition per Sample (trimmed, mapped reads)",
    y = "% Modified Bases (Pass Fraction)",
    x = NULL
  )

```


## genome coverage summaries

```{r, engine='bash', eval=FALSE}
source .bashvars
cd ${output_dir_top}

# conda create -n myenv
conda activate mosdepth_env
conda config --add channels bioconda
conda config --add channels conda-forge
conda config --add channels defaults
conda install -c bioconda mosdepth
mosdepth --version

```

